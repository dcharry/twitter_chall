{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparacion Ambiente de trabajo.\n",
    "\n",
    "1. Entorno de trabajo: La solucion del desafio es desarrollada en un workbench de Vertex AI, la instancia utilizada es una e2-standar-2 de 2 CPU virtuales y 8 GB de RAM con python3 y conda previamente instalado.\n",
    "\n",
    "2. Para el manejo de versiones del desarrollo, se utilizara la herramienta GitHub, para la configuracion de este ambiente de trabajo se siguio los siguientes pasos:\n",
    "\n",
    "    a. Se descargo y se descomprimio la carpeta challenge_DE, la cual contiene la estructura del proyecto, brindada por el equipo de LATAM.\n",
    "    \n",
    "    b. Inicializacion GIT en la carpeta del proyecto: \n",
    "    - *git init*\n",
    "    \n",
    "    \n",
    "    c. Agregar los archivos al repositorio:\n",
    "    - *git add .*\n",
    "    \n",
    "    d. Realizacion del primer commit para registrar los archivos iniciales de repo:\n",
    "    - *git commit -m \"Initial commit w/ project skeleton*\n",
    "    \n",
    "    \n",
    "    e. Atenticacion con mi cuenta de GitHub personal:\n",
    "    - *gh auth login*\n",
    "    \n",
    "    \n",
    "    f. Creacion de un repositorio en GitHub, publico.\n",
    "    - *gh repo create twitter_chall --public --confirm*\n",
    "    \n",
    "    \n",
    "    g. Coneccion de mi repositorio local al repositorio remoto y realizacion del primer push de los cambios.\n",
    "    - *git remote add origin https://github.com/dcharry/twitter_chall.git*\n",
    "    - *git branch -M main*\n",
    "    - *git push -u origin main*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura data de twitter\n",
    "\n",
    "1. En la terminal se installo la biblioteca gdwon para descargar el archivo desde google drive\n",
    "\n",
    "2. Se creo la carpeta src/data para descargar la data a trabajar.\n",
    "\n",
    "3. se corre el comando 'gdwon --id' para descargar el archivo usando el ID del drive, el cual es '1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis' \n",
    "\n",
    "\n",
    "4. se descomprime el archivo utilizando el comando unzip en la terminal.\n",
    "\n",
    "5. El contenido es un archivo json, se utiliza la libreria pandas para abrir el archivo en formato panda.DataFrame y explorar su contenido. (Primer acercamiento a la data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('data/farmers-protest-tweets-2021-2-4.json', lines = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>outlinks</th>\n",
       "      <th>tcooutlinks</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>...</th>\n",
       "      <th>quoteCount</th>\n",
       "      <th>conversationId</th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>sourceLabel</th>\n",
       "      <th>media</th>\n",
       "      <th>retweetedTweet</th>\n",
       "      <th>quotedTweet</th>\n",
       "      <th>mentionedUsers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://twitter.com/ArjunSinghPanam/status/136...</td>\n",
       "      <td>2021-02-24 09:23:35+00:00</td>\n",
       "      <td>The world progresses while the Indian police a...</td>\n",
       "      <td>The world progresses while the Indian police a...</td>\n",
       "      <td>1364506249291784198</td>\n",
       "      <td>{'username': 'ArjunSinghPanam', 'displayname':...</td>\n",
       "      <td>[https://twitter.com/ravisinghka/status/136415...</td>\n",
       "      <td>[https://t.co/es3kn0IQAF]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1364506249291784198</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'url': 'https://twitter.com/RaviSinghKA/statu...</td>\n",
       "      <td>[{'username': 'narendramodi', 'displayname': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/PrdeepNain/status/13645062...</td>\n",
       "      <td>2021-02-24 09:23:32+00:00</td>\n",
       "      <td>#FarmersProtest \\n#ModiIgnoringFarmersDeaths \\...</td>\n",
       "      <td>#FarmersProtest \\n#ModiIgnoringFarmersDeaths \\...</td>\n",
       "      <td>1364506237451313155</td>\n",
       "      <td>{'username': 'PrdeepNain', 'displayname': 'Pra...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1364506237451313155</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>[{'thumbnailUrl': 'https://pbs.twimg.com/ext_t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'username': 'Kisanektamorcha', 'displayname'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://twitter.com/parmarmaninder/status/1364...</td>\n",
       "      <td>2021-02-24 09:23:22+00:00</td>\n",
       "      <td>ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾਂ ਨੂੰ ਮੱਦੇਨਜ਼ਰ ਰੱਖਦੇ ਹੋਏ \\nਮੇ...</td>\n",
       "      <td>ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾਂ ਨੂੰ ਮੱਦੇਨਜ਼ਰ ਰੱਖਦੇ ਹੋਏ \\nਮੇ...</td>\n",
       "      <td>1364506195453767680</td>\n",
       "      <td>{'username': 'parmarmaninder', 'displayname': ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1364506195453767680</td>\n",
       "      <td>pa</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://twitter.com/anmoldhaliwal/status/13645...</td>\n",
       "      <td>2021-02-24 09:23:16+00:00</td>\n",
       "      <td>@ReallySwara @rohini_sgh watch full video here...</td>\n",
       "      <td>@ReallySwara @rohini_sgh watch full video here...</td>\n",
       "      <td>1364506167226032128</td>\n",
       "      <td>{'username': 'anmoldhaliwal', 'displayname': '...</td>\n",
       "      <td>[https://youtu.be/-bUKumwq-J8]</td>\n",
       "      <td>[https://t.co/wBPNdJdB0n]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1364350947099484160</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>https://mobile.twitter.com</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>[{'thumbnailUrl': 'https://pbs.twimg.com/ext_t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'username': 'ReallySwara', 'displayname': 'S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://twitter.com/KotiaPreet/status/13645061...</td>\n",
       "      <td>2021-02-24 09:23:10+00:00</td>\n",
       "      <td>#KisanEktaMorcha #FarmersProtest #NoFarmersNoF...</td>\n",
       "      <td>#KisanEktaMorcha #FarmersProtest #NoFarmersNoF...</td>\n",
       "      <td>1364506144002088963</td>\n",
       "      <td>{'username': 'KotiaPreet', 'displayname': 'Pre...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1364506144002088963</td>\n",
       "      <td>und</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>[{'previewUrl': 'https://pbs.twimg.com/media/E...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://twitter.com/ArjunSinghPanam/status/136...   \n",
       "1  https://twitter.com/PrdeepNain/status/13645062...   \n",
       "2  https://twitter.com/parmarmaninder/status/1364...   \n",
       "3  https://twitter.com/anmoldhaliwal/status/13645...   \n",
       "4  https://twitter.com/KotiaPreet/status/13645061...   \n",
       "\n",
       "                       date  \\\n",
       "0 2021-02-24 09:23:35+00:00   \n",
       "1 2021-02-24 09:23:32+00:00   \n",
       "2 2021-02-24 09:23:22+00:00   \n",
       "3 2021-02-24 09:23:16+00:00   \n",
       "4 2021-02-24 09:23:10+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  The world progresses while the Indian police a...   \n",
       "1  #FarmersProtest \\n#ModiIgnoringFarmersDeaths \\...   \n",
       "2  ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾਂ ਨੂੰ ਮੱਦੇਨਜ਼ਰ ਰੱਖਦੇ ਹੋਏ \\nਮੇ...   \n",
       "3  @ReallySwara @rohini_sgh watch full video here...   \n",
       "4  #KisanEktaMorcha #FarmersProtest #NoFarmersNoF...   \n",
       "\n",
       "                                     renderedContent                   id  \\\n",
       "0  The world progresses while the Indian police a...  1364506249291784198   \n",
       "1  #FarmersProtest \\n#ModiIgnoringFarmersDeaths \\...  1364506237451313155   \n",
       "2  ਪੈਟਰੋਲ ਦੀਆਂ ਕੀਮਤਾਂ ਨੂੰ ਮੱਦੇਨਜ਼ਰ ਰੱਖਦੇ ਹੋਏ \\nਮੇ...  1364506195453767680   \n",
       "3  @ReallySwara @rohini_sgh watch full video here...  1364506167226032128   \n",
       "4  #KisanEktaMorcha #FarmersProtest #NoFarmersNoF...  1364506144002088963   \n",
       "\n",
       "                                                user  \\\n",
       "0  {'username': 'ArjunSinghPanam', 'displayname':...   \n",
       "1  {'username': 'PrdeepNain', 'displayname': 'Pra...   \n",
       "2  {'username': 'parmarmaninder', 'displayname': ...   \n",
       "3  {'username': 'anmoldhaliwal', 'displayname': '...   \n",
       "4  {'username': 'KotiaPreet', 'displayname': 'Pre...   \n",
       "\n",
       "                                            outlinks  \\\n",
       "0  [https://twitter.com/ravisinghka/status/136415...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                     [https://youtu.be/-bUKumwq-J8]   \n",
       "4                                                 []   \n",
       "\n",
       "                 tcooutlinks  replyCount  retweetCount  ...  quoteCount  \\\n",
       "0  [https://t.co/es3kn0IQAF]           0             0  ...           0   \n",
       "1                         []           0             0  ...           0   \n",
       "2                         []           0             0  ...           0   \n",
       "3  [https://t.co/wBPNdJdB0n]           0             0  ...           0   \n",
       "4                         []           0             0  ...           0   \n",
       "\n",
       "        conversationId  lang  \\\n",
       "0  1364506249291784198    en   \n",
       "1  1364506237451313155    en   \n",
       "2  1364506195453767680    pa   \n",
       "3  1364350947099484160    en   \n",
       "4  1364506144002088963   und   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...   \n",
       "2  <a href=\"http://twitter.com/download/android\" ...   \n",
       "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                             sourceUrl          sourceLabel  \\\n",
       "0   http://twitter.com/download/iphone   Twitter for iPhone   \n",
       "1  http://twitter.com/download/android  Twitter for Android   \n",
       "2  http://twitter.com/download/android  Twitter for Android   \n",
       "3           https://mobile.twitter.com      Twitter Web App   \n",
       "4   http://twitter.com/download/iphone   Twitter for iPhone   \n",
       "\n",
       "                                               media retweetedTweet  \\\n",
       "0                                               None            NaN   \n",
       "1  [{'thumbnailUrl': 'https://pbs.twimg.com/ext_t...            NaN   \n",
       "2                                               None            NaN   \n",
       "3  [{'thumbnailUrl': 'https://pbs.twimg.com/ext_t...            NaN   \n",
       "4  [{'previewUrl': 'https://pbs.twimg.com/media/E...            NaN   \n",
       "\n",
       "                                         quotedTweet  \\\n",
       "0  {'url': 'https://twitter.com/RaviSinghKA/statu...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                      mentionedUsers  \n",
       "0  [{'username': 'narendramodi', 'displayname': '...  \n",
       "1  [{'username': 'Kisanektamorcha', 'displayname'...  \n",
       "2                                               None  \n",
       "3  [{'username': 'ReallySwara', 'displayname': 'S...  \n",
       "4                                               None  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>quoteCount</th>\n",
       "      <th>conversationId</th>\n",
       "      <th>retweetedTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.174070e+05</td>\n",
       "      <td>117407.000000</td>\n",
       "      <td>117407.000000</td>\n",
       "      <td>117407.000000</td>\n",
       "      <td>117407.000000</td>\n",
       "      <td>1.174070e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.362062e+18</td>\n",
       "      <td>0.631410</td>\n",
       "      <td>5.045619</td>\n",
       "      <td>14.609674</td>\n",
       "      <td>0.336241</td>\n",
       "      <td>1.361764e+18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.293803e+15</td>\n",
       "      <td>9.436159</td>\n",
       "      <td>58.237370</td>\n",
       "      <td>229.083983</td>\n",
       "      <td>4.474576</td>\n",
       "      <td>1.294352e+16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.360040e+18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.981606e+10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.360935e+18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.360887e+18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.361945e+18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.361914e+18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.363097e+18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.363068e+18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.364506e+18</td>\n",
       "      <td>1291.000000</td>\n",
       "      <td>7723.000000</td>\n",
       "      <td>27888.000000</td>\n",
       "      <td>756.000000</td>\n",
       "      <td>1.364506e+18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     replyCount   retweetCount      likeCount  \\\n",
       "count  1.174070e+05  117407.000000  117407.000000  117407.000000   \n",
       "mean   1.362062e+18       0.631410       5.045619      14.609674   \n",
       "std    1.293803e+15       9.436159      58.237370     229.083983   \n",
       "min    1.360040e+18       0.000000       0.000000       0.000000   \n",
       "25%    1.360935e+18       0.000000       0.000000       0.000000   \n",
       "50%    1.361945e+18       0.000000       0.000000       1.000000   \n",
       "75%    1.363097e+18       0.000000       1.000000       2.000000   \n",
       "max    1.364506e+18    1291.000000    7723.000000   27888.000000   \n",
       "\n",
       "          quoteCount  conversationId  retweetedTweet  \n",
       "count  117407.000000    1.174070e+05             0.0  \n",
       "mean        0.336241    1.361764e+18             NaN  \n",
       "std         4.474576    1.294352e+16             NaN  \n",
       "min         0.000000    1.981606e+10             NaN  \n",
       "25%         0.000000    1.360887e+18             NaN  \n",
       "50%         0.000000    1.361914e+18             NaN  \n",
       "75%         0.000000    1.363068e+18             NaN  \n",
       "max       756.000000    1.364506e+18             NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-procesamiento Data\n",
    "\n",
    "En esta seccion se abora una etapa de exploracion y pre-procesamiento de la data, para entender sus valores, normalizar la data, hacer limpieza de ser necesario y demas, una vez la etapa de exploracion sea concluida, podemos crear un modulo, de ser necesario, encargado de ejecutar las funciones necesarias para el pre-procesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punto 1.\n",
    "\n",
    "- Para abarcar el punto 1 se requiere trabajar con la columna 'date' y 'user' para extraer el top 10 de fechas con mas tweets y el ususario con mas publicaciones \n",
    "\n",
    "Iniciaremos trabajando con la columna date, procesandola en pandas y convirtiendola en formato DataTime.\n",
    "\n",
    "**Nota:** Todo el analisis exploratorio se realizaran en pandas, no se hara pensado en optimizar memoria debido a que es el primer acercamiento a la data, analizando las problematicas y columnas involucradas en cada una, para la deteccion de algun tipo de transfomacion en la data de ser requerido, esta exploracion no es la solucion final de cada uno de los puntos, esta solucion sera realizada en los modulos de python correspondientes, sin embargo se deja el analisis dentro del notebook por motivos de seguimiento de pensamiento logico de la solucion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2021-02-24\n",
       "1         2021-02-24\n",
       "2         2021-02-24\n",
       "3         2021-02-24\n",
       "4         2021-02-24\n",
       "             ...    \n",
       "117402    2021-02-12\n",
       "117403    2021-02-12\n",
       "117404    2021-02-12\n",
       "117405    2021-02-12\n",
       "117406    2021-02-12\n",
       "Name: newDate, Length: 117407, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['newDate']= pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "df['newDate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Utilizacion de funcion **value_counts** de pandas para realizar un conteo de la frecuencia de cada una de las fechas y asi optener el top 10*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_dates= df['newDate'].value_counts().head(10).index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([2021-02-12, 2021-02-13, 2021-02-17, 2021-02-16, 2021-02-14, 2021-02-18,\n",
       "       2021-02-15, 2021-02-20, 2021-02-23, 2021-02-19],\n",
       "      dtype='object', name='newDate')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'username': 'ArjunSinghPanam',\n",
       " 'displayname': 'Arjun Singh Panam',\n",
       " 'id': 45091142,\n",
       " 'description': 'Global Citizen, Actor, Director: Sky is the roof above my head, the world is the road I travel, love is my food & mother earth is my bed. Roy in @CosmosMovie',\n",
       " 'rawDescription': 'Global Citizen, Actor, Director: Sky is the roof above my head, the world is the road I travel, love is my food & mother earth is my bed. Roy in @CosmosMovie',\n",
       " 'descriptionUrls': [],\n",
       " 'verified': False,\n",
       " 'created': '2009-06-06T07:50:57+00:00',\n",
       " 'followersCount': 603,\n",
       " 'friendsCount': 311,\n",
       " 'statusesCount': 17534,\n",
       " 'favouritesCount': 4269,\n",
       " 'listedCount': 23,\n",
       " 'mediaCount': 1211,\n",
       " 'location': '',\n",
       " 'protected': False,\n",
       " 'linkUrl': 'https://www.cosmosmovieofficial.com',\n",
       " 'linkTcourl': 'https://t.co/3uaoV3gCt3',\n",
       " 'profileImageUrl': 'https://pbs.twimg.com/profile_images/1215541746492461056/3De61YoQ_normal.jpg',\n",
       " 'profileBannerUrl': 'https://pbs.twimg.com/profile_banners/45091142/1612601766',\n",
       " 'url': 'https://twitter.com/ArjunSinghPanam'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['user'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Se observa que la columna **user** es un diccionario de datos, donde el indice **username** contiene el nombre del usuario. Con este hallazgo en mente, podemos filtrar el DataFrame por una de las fechas dentro del top 10, obteniendo asi los tweets para ese dia en especifico, luego hacer uso de la funcion **lambda** de python para recorrer cada usuario dentro de la columna **user**, extrayendo su nombre y realizando un conteo de frecuencia, de nuevo con la funcion **value_counts***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_df= df[df['newDate'] == top_dates[0]]\n",
    "top_user2 = sub_df['user'].apply(lambda x: x['username']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user\n",
       "RanbirS00614606    176\n",
       "ammiedgr8          116\n",
       "rebelpacifist      109\n",
       "lovehazran1         94\n",
       "Davinder_001        82\n",
       "                  ... \n",
       "imayanmajumdar       1\n",
       "Nirbhanyadaviyc      1\n",
       "AnoopChanot          1\n",
       "HindiNews18          1\n",
       "rickyrickstir        1\n",
       "Name: count, Length: 5817, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_user2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>outlinks</th>\n",
       "      <th>tcooutlinks</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>...</th>\n",
       "      <th>conversationId</th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>sourceLabel</th>\n",
       "      <th>media</th>\n",
       "      <th>retweetedTweet</th>\n",
       "      <th>quotedTweet</th>\n",
       "      <th>mentionedUsers</th>\n",
       "      <th>newDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105060</th>\n",
       "      <td>https://twitter.com/Aman96442728/status/136037...</td>\n",
       "      <td>2021-02-12 23:59:59+00:00</td>\n",
       "      <td>#FarmersProtest</td>\n",
       "      <td>#FarmersProtest</td>\n",
       "      <td>1360378149003874305</td>\n",
       "      <td>{'username': 'Aman96442728', 'displayname': 'A...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1360378149003874305</td>\n",
       "      <td>und</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105061</th>\n",
       "      <td>https://twitter.com/Gurpreetd86/status/1360378...</td>\n",
       "      <td>2021-02-12 23:59:57+00:00</td>\n",
       "      <td>#nodeepkaur #freejagginow #MahapanchayatRevolu...</td>\n",
       "      <td>#nodeepkaur #freejagginow #MahapanchayatRevolu...</td>\n",
       "      <td>1360378137046118400</td>\n",
       "      <td>{'username': 'Gurpreetd86', 'displayname': 'Gu...</td>\n",
       "      <td>[https://twitter.com/UK51NGH/status/1360377290...</td>\n",
       "      <td>[https://t.co/haAroa5swm]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1360378137046118400</td>\n",
       "      <td>und</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'url': 'https://twitter.com/UK51NGH/status/13...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105062</th>\n",
       "      <td>https://twitter.com/Gurpreetd86/status/1360378...</td>\n",
       "      <td>2021-02-12 23:59:40+00:00</td>\n",
       "      <td>@BaazNewsOrg #nodeepkaur #freejagginow #Mahapa...</td>\n",
       "      <td>@BaazNewsOrg #nodeepkaur #freejagginow #Mahapa...</td>\n",
       "      <td>1360378065499602947</td>\n",
       "      <td>{'username': 'Gurpreetd86', 'displayname': 'Gu...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1360308284922728451</td>\n",
       "      <td>und</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'username': 'BaazNewsOrg', 'displayname': 'B...</td>\n",
       "      <td>2021-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105063</th>\n",
       "      <td>https://twitter.com/Gurpreetd86/status/1360377...</td>\n",
       "      <td>2021-02-12 23:59:16+00:00</td>\n",
       "      <td>#nodeepkaur #freejagginow #MahapanchayatRevolu...</td>\n",
       "      <td>#nodeepkaur #freejagginow #MahapanchayatRevolu...</td>\n",
       "      <td>1360377968699338754</td>\n",
       "      <td>{'username': 'Gurpreetd86', 'displayname': 'Gu...</td>\n",
       "      <td>[https://twitter.com/BaazNewsOrg/status/136030...</td>\n",
       "      <td>[https://t.co/gy7cpgwHWK]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1360377968699338754</td>\n",
       "      <td>und</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'url': 'https://twitter.com/BaazNewsOrg/statu...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105064</th>\n",
       "      <td>https://twitter.com/gpd_bhangra/status/1360377...</td>\n",
       "      <td>2021-02-12 23:59:07+00:00</td>\n",
       "      <td>#FarmersProtest https://t.co/5PArZdboy0</td>\n",
       "      <td>#FarmersProtest https://t.co/5PArZdboy0</td>\n",
       "      <td>1360377930040430594</td>\n",
       "      <td>{'username': 'gpd_bhangra', 'displayname': 'Ga...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1360377930040430594</td>\n",
       "      <td>und</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>[{'previewUrl': 'https://pbs.twimg.com/media/E...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117402</th>\n",
       "      <td>https://twitter.com/rickyrickstir/status/13600...</td>\n",
       "      <td>2021-02-12 01:37:02+00:00</td>\n",
       "      <td>#FarmersProtest #KisanAndolan #KisaanMajdoorEk...</td>\n",
       "      <td>#FarmersProtest #KisanAndolan #KisaanMajdoorEk...</td>\n",
       "      <td>1360040182771163138</td>\n",
       "      <td>{'username': 'rickyrickstir', 'displayname': '...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1360040182771163138</td>\n",
       "      <td>und</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117403</th>\n",
       "      <td>https://twitter.com/PunjabTak/status/136004014...</td>\n",
       "      <td>2021-02-12 01:36:53+00:00</td>\n",
       "      <td>PM मोदी की अपील के बीच संयुक्त किसान मोर्चा का...</td>\n",
       "      <td>PM मोदी की अपील के बीच संयुक्त किसान मोर्चा का...</td>\n",
       "      <td>1360040146402373637</td>\n",
       "      <td>{'username': 'PunjabTak', 'displayname': 'Punj...</td>\n",
       "      <td>[https://youtu.be/aG3qHGwoYag]</td>\n",
       "      <td>[https://t.co/AzZNOGI8BX]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1360040146402373637</td>\n",
       "      <td>hi</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>https://mobile.twitter.com</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>[{'previewUrl': 'https://pbs.twimg.com/media/E...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117404</th>\n",
       "      <td>https://twitter.com/ish_kayy/status/1360040134...</td>\n",
       "      <td>2021-02-12 01:36:50+00:00</td>\n",
       "      <td>United we stand.\\nDivided we fall\\n#Mahapancha...</td>\n",
       "      <td>United we stand.\\nDivided we fall\\n#Mahapancha...</td>\n",
       "      <td>1360040134230556678</td>\n",
       "      <td>{'username': 'ish_kayy', 'displayname': 'ishy'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>1360040134230556678</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>https://mobile.twitter.com</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>[{'previewUrl': 'https://pbs.twimg.com/media/E...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117405</th>\n",
       "      <td>https://twitter.com/TV9Bharatvarsh/status/1360...</td>\n",
       "      <td>2021-02-12 01:36:49+00:00</td>\n",
       "      <td>सिंघु बॉर्डर पर लंबी लड़ाई की तैयारी, किसानों ...</td>\n",
       "      <td>सिंघु बॉर्डर पर लंबी लड़ाई की तैयारी, किसानों ...</td>\n",
       "      <td>1360040127679000577</td>\n",
       "      <td>{'username': 'TV9Bharatvarsh', 'displayname': ...</td>\n",
       "      <td>[https://www.tv9hindi.com/india/farmers-protes...</td>\n",
       "      <td>[https://t.co/bkjh7WXc0w]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1360040127679000577</td>\n",
       "      <td>hi</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>https://mobile.twitter.com</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117406</th>\n",
       "      <td>https://twitter.com/SikhVibes/status/136004012...</td>\n",
       "      <td>2021-02-12 01:36:49+00:00</td>\n",
       "      <td>@Kisanektamorcha We are with you, keep the mor...</td>\n",
       "      <td>@Kisanektamorcha We are with you, keep the mor...</td>\n",
       "      <td>1360040127146430470</td>\n",
       "      <td>{'username': 'SikhVibes', 'displayname': 'Sikh...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>1360038291471388672</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'username': 'Kisanektamorcha', 'displayname'...</td>\n",
       "      <td>2021-02-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12347 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  \\\n",
       "105060  https://twitter.com/Aman96442728/status/136037...   \n",
       "105061  https://twitter.com/Gurpreetd86/status/1360378...   \n",
       "105062  https://twitter.com/Gurpreetd86/status/1360378...   \n",
       "105063  https://twitter.com/Gurpreetd86/status/1360377...   \n",
       "105064  https://twitter.com/gpd_bhangra/status/1360377...   \n",
       "...                                                   ...   \n",
       "117402  https://twitter.com/rickyrickstir/status/13600...   \n",
       "117403  https://twitter.com/PunjabTak/status/136004014...   \n",
       "117404  https://twitter.com/ish_kayy/status/1360040134...   \n",
       "117405  https://twitter.com/TV9Bharatvarsh/status/1360...   \n",
       "117406  https://twitter.com/SikhVibes/status/136004012...   \n",
       "\n",
       "                            date  \\\n",
       "105060 2021-02-12 23:59:59+00:00   \n",
       "105061 2021-02-12 23:59:57+00:00   \n",
       "105062 2021-02-12 23:59:40+00:00   \n",
       "105063 2021-02-12 23:59:16+00:00   \n",
       "105064 2021-02-12 23:59:07+00:00   \n",
       "...                          ...   \n",
       "117402 2021-02-12 01:37:02+00:00   \n",
       "117403 2021-02-12 01:36:53+00:00   \n",
       "117404 2021-02-12 01:36:50+00:00   \n",
       "117405 2021-02-12 01:36:49+00:00   \n",
       "117406 2021-02-12 01:36:49+00:00   \n",
       "\n",
       "                                                  content  \\\n",
       "105060                                    #FarmersProtest   \n",
       "105061  #nodeepkaur #freejagginow #MahapanchayatRevolu...   \n",
       "105062  @BaazNewsOrg #nodeepkaur #freejagginow #Mahapa...   \n",
       "105063  #nodeepkaur #freejagginow #MahapanchayatRevolu...   \n",
       "105064            #FarmersProtest https://t.co/5PArZdboy0   \n",
       "...                                                   ...   \n",
       "117402  #FarmersProtest #KisanAndolan #KisaanMajdoorEk...   \n",
       "117403  PM मोदी की अपील के बीच संयुक्त किसान मोर्चा का...   \n",
       "117404  United we stand.\\nDivided we fall\\n#Mahapancha...   \n",
       "117405  सिंघु बॉर्डर पर लंबी लड़ाई की तैयारी, किसानों ...   \n",
       "117406  @Kisanektamorcha We are with you, keep the mor...   \n",
       "\n",
       "                                          renderedContent  \\\n",
       "105060                                    #FarmersProtest   \n",
       "105061  #nodeepkaur #freejagginow #MahapanchayatRevolu...   \n",
       "105062  @BaazNewsOrg #nodeepkaur #freejagginow #Mahapa...   \n",
       "105063  #nodeepkaur #freejagginow #MahapanchayatRevolu...   \n",
       "105064            #FarmersProtest https://t.co/5PArZdboy0   \n",
       "...                                                   ...   \n",
       "117402  #FarmersProtest #KisanAndolan #KisaanMajdoorEk...   \n",
       "117403  PM मोदी की अपील के बीच संयुक्त किसान मोर्चा का...   \n",
       "117404  United we stand.\\nDivided we fall\\n#Mahapancha...   \n",
       "117405  सिंघु बॉर्डर पर लंबी लड़ाई की तैयारी, किसानों ...   \n",
       "117406  @Kisanektamorcha We are with you, keep the mor...   \n",
       "\n",
       "                         id  \\\n",
       "105060  1360378149003874305   \n",
       "105061  1360378137046118400   \n",
       "105062  1360378065499602947   \n",
       "105063  1360377968699338754   \n",
       "105064  1360377930040430594   \n",
       "...                     ...   \n",
       "117402  1360040182771163138   \n",
       "117403  1360040146402373637   \n",
       "117404  1360040134230556678   \n",
       "117405  1360040127679000577   \n",
       "117406  1360040127146430470   \n",
       "\n",
       "                                                     user  \\\n",
       "105060  {'username': 'Aman96442728', 'displayname': 'A...   \n",
       "105061  {'username': 'Gurpreetd86', 'displayname': 'Gu...   \n",
       "105062  {'username': 'Gurpreetd86', 'displayname': 'Gu...   \n",
       "105063  {'username': 'Gurpreetd86', 'displayname': 'Gu...   \n",
       "105064  {'username': 'gpd_bhangra', 'displayname': 'Ga...   \n",
       "...                                                   ...   \n",
       "117402  {'username': 'rickyrickstir', 'displayname': '...   \n",
       "117403  {'username': 'PunjabTak', 'displayname': 'Punj...   \n",
       "117404  {'username': 'ish_kayy', 'displayname': 'ishy'...   \n",
       "117405  {'username': 'TV9Bharatvarsh', 'displayname': ...   \n",
       "117406  {'username': 'SikhVibes', 'displayname': 'Sikh...   \n",
       "\n",
       "                                                 outlinks  \\\n",
       "105060                                                 []   \n",
       "105061  [https://twitter.com/UK51NGH/status/1360377290...   \n",
       "105062                                                 []   \n",
       "105063  [https://twitter.com/BaazNewsOrg/status/136030...   \n",
       "105064                                                 []   \n",
       "...                                                   ...   \n",
       "117402                                                 []   \n",
       "117403                     [https://youtu.be/aG3qHGwoYag]   \n",
       "117404                                                 []   \n",
       "117405  [https://www.tv9hindi.com/india/farmers-protes...   \n",
       "117406                                                 []   \n",
       "\n",
       "                      tcooutlinks  replyCount  retweetCount  ...  \\\n",
       "105060                         []           0             0  ...   \n",
       "105061  [https://t.co/haAroa5swm]           0             0  ...   \n",
       "105062                         []           0             0  ...   \n",
       "105063  [https://t.co/gy7cpgwHWK]           0             0  ...   \n",
       "105064                         []           1             0  ...   \n",
       "...                           ...         ...           ...  ...   \n",
       "117402                         []           0             0  ...   \n",
       "117403  [https://t.co/AzZNOGI8BX]           0             0  ...   \n",
       "117404                         []           0            65  ...   \n",
       "117405  [https://t.co/bkjh7WXc0w]           0             1  ...   \n",
       "117406                         []           2            19  ...   \n",
       "\n",
       "             conversationId  lang  \\\n",
       "105060  1360378149003874305   und   \n",
       "105061  1360378137046118400   und   \n",
       "105062  1360308284922728451   und   \n",
       "105063  1360377968699338754   und   \n",
       "105064  1360377930040430594   und   \n",
       "...                     ...   ...   \n",
       "117402  1360040182771163138   und   \n",
       "117403  1360040146402373637    hi   \n",
       "117404  1360040134230556678    en   \n",
       "117405  1360040127679000577    hi   \n",
       "117406  1360038291471388672    en   \n",
       "\n",
       "                                                   source  \\\n",
       "105060  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "105061  <a href=\"http://twitter.com/download/android\" ...   \n",
       "105062  <a href=\"http://twitter.com/download/android\" ...   \n",
       "105063  <a href=\"http://twitter.com/download/android\" ...   \n",
       "105064  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "...                                                   ...   \n",
       "117402  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "117403  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "117404  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "117405  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "117406  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                                  sourceUrl          sourceLabel  \\\n",
       "105060   http://twitter.com/download/iphone   Twitter for iPhone   \n",
       "105061  http://twitter.com/download/android  Twitter for Android   \n",
       "105062  http://twitter.com/download/android  Twitter for Android   \n",
       "105063  http://twitter.com/download/android  Twitter for Android   \n",
       "105064   http://twitter.com/download/iphone   Twitter for iPhone   \n",
       "...                                     ...                  ...   \n",
       "117402   http://twitter.com/download/iphone   Twitter for iPhone   \n",
       "117403           https://mobile.twitter.com      Twitter Web App   \n",
       "117404           https://mobile.twitter.com      Twitter Web App   \n",
       "117405           https://mobile.twitter.com      Twitter Web App   \n",
       "117406   http://twitter.com/download/iphone   Twitter for iPhone   \n",
       "\n",
       "                                                    media retweetedTweet  \\\n",
       "105060                                               None            NaN   \n",
       "105061                                               None            NaN   \n",
       "105062                                               None            NaN   \n",
       "105063                                               None            NaN   \n",
       "105064  [{'previewUrl': 'https://pbs.twimg.com/media/E...            NaN   \n",
       "...                                                   ...            ...   \n",
       "117402                                               None            NaN   \n",
       "117403  [{'previewUrl': 'https://pbs.twimg.com/media/E...            NaN   \n",
       "117404  [{'previewUrl': 'https://pbs.twimg.com/media/E...            NaN   \n",
       "117405                                               None            NaN   \n",
       "117406                                               None            NaN   \n",
       "\n",
       "                                              quotedTweet  \\\n",
       "105060                                               None   \n",
       "105061  {'url': 'https://twitter.com/UK51NGH/status/13...   \n",
       "105062                                               None   \n",
       "105063  {'url': 'https://twitter.com/BaazNewsOrg/statu...   \n",
       "105064                                               None   \n",
       "...                                                   ...   \n",
       "117402                                               None   \n",
       "117403                                               None   \n",
       "117404                                               None   \n",
       "117405                                               None   \n",
       "117406                                               None   \n",
       "\n",
       "                                           mentionedUsers     newDate  \n",
       "105060                                               None  2021-02-12  \n",
       "105061                                               None  2021-02-12  \n",
       "105062  [{'username': 'BaazNewsOrg', 'displayname': 'B...  2021-02-12  \n",
       "105063                                               None  2021-02-12  \n",
       "105064                                               None  2021-02-12  \n",
       "...                                                   ...         ...  \n",
       "117402                                               None  2021-02-12  \n",
       "117403                                               None  2021-02-12  \n",
       "117404                                               None  2021-02-12  \n",
       "117405                                               None  2021-02-12  \n",
       "117406  [{'username': 'Kisanektamorcha', 'displayname'...  2021-02-12  \n",
       "\n",
       "[12347 rows x 22 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ahora podemos realizar un pequeña generalizacion de la solucion, para obtener el top 10 de fechas con mas tweets y los usuario con mas tweets en ese dia, para ello podemos hacer uso del ciclo for:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-12\n",
      "12347\n",
      "2021-02-13\n",
      "11296\n",
      "2021-02-17\n",
      "11087\n",
      "2021-02-16\n",
      "10443\n",
      "2021-02-14\n",
      "10249\n",
      "2021-02-18\n",
      "9625\n",
      "2021-02-15\n",
      "9197\n",
      "2021-02-20\n",
      "8502\n",
      "2021-02-23\n",
      "8417\n",
      "2021-02-19\n",
      "8204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result =[]\n",
    "for date in top_dates:\n",
    "    #filtro el DataFrame con las fechas especificas \n",
    "    print(date)\n",
    "    sub_df= df[df['newDate'] == date]\n",
    "    print(len(sub_df))\n",
    "    \n",
    "    #encontrar el ususario con mas tweets para la fecha dada\n",
    "    top_user = sub_df['user'].apply(lambda x: x['username']).value_counts().idxmax()\n",
    "    \n",
    "    result.append((date, top_user))\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punto 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para abarcar el punto 2, se requiere analizar la columna 'content' la cual contiene el texto de tweet, en esta columna podemos encontrar los emojis utilizados en cada uno de los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#FarmersProtest \\n#ModiIgnoringFarmersDeaths \\n#ModiDontSellFarmers \\n@Kisanektamorcha \\nFarmers constantly distroying crops throughout India. \\nReally, it's hearts breaking...we care about our crops like our children. And govt. agriculture minister is laughing on us🚜🌾WE WILL WIN💪 https://t.co/kLspngG9xE\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet=df['content'][1]\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para solucionar la problematica, analizando el contenido del texto, se me ocurren dos posibles solciones:\n",
    "\n",
    "1. Utilizar expresiones regulares para encontrar los emojis, esto implica crear un diccionario con cada una de las expresiones regulares que me definen un emoji.\n",
    "\n",
    "2. Usar la biblioteca de python llamada 'emoji', es una biblioteca que me ayuda a la deteccion de emojis en un texto, tengo conocimiento de esta biblioteca debido a que en una de mis experiencias laborales trabaje en un proyecto de scraping y analisis de comentarios en Instagram, esta biblioteca en su momento abarcaba una mayor cantidad de emojis de las que se pueden generalizar por regex, obteniendo un muy buen performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#expresiones regulares \n",
    "import re\n",
    "\n",
    "# Expresión regular para encontrar emojis\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "df['emojis'] = df['content'].apply(lambda x: re.findall(emoji_pattern, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['🚜🌾', '💪']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emojis'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         NaN\n",
       "1          🚜🌾\n",
       "1           💪\n",
       "2         NaN\n",
       "3         NaN\n",
       "         ... \n",
       "117402    NaN\n",
       "117403    NaN\n",
       "117404    NaN\n",
       "117405    NaN\n",
       "117406      💪\n",
       "Name: emojis, Length: 122083, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_emojis = df['emojis'].explode()\n",
    "all_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1         🚜🌾\n",
       "1          💪\n",
       "21        🙄🙄\n",
       "29        🇮🇳\n",
       "41        👇👇\n",
       "          ..\n",
       "117344     🌾\n",
       "117368     🛑\n",
       "117371     💯\n",
       "117395    🇮🇳\n",
       "117406     💪\n",
       "Name: emojis, Length: 18539, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_emojis[pd.notna(all_emojis)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emojis\n",
       "🙏     2123\n",
       "😂      633\n",
       "🌾      605\n",
       "💚      533\n",
       "👍      471\n",
       "👉      450\n",
       "🇮🇳     426\n",
       "🙏🙏     403\n",
       "👇      390\n",
       "🏽      332\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_emojis.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in /opt/conda/lib/python3.10/site-packages (from emoji) (4.12.2)\n",
      "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: emoji\n",
      "Successfully installed emoji-2.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "# Función para extraer emojis\n",
    "def extract_emojis(text):\n",
    "    return [char for char in text if emoji.is_emoji(char)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extraer y contar emojis\n",
    "df['emojis'] = df['content'].apply(extract_emojis)\n",
    "all_emojis = df['emojis'].explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         NaN\n",
       "1           🚜\n",
       "1           🌾\n",
       "1           💪\n",
       "2           🤫\n",
       "         ... \n",
       "117402    NaN\n",
       "117403    NaN\n",
       "117404    NaN\n",
       "117405    NaN\n",
       "117406      💪\n",
       "Name: emojis, Length: 146482, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1         🚜\n",
       "1         🌾\n",
       "1         💪\n",
       "2         🤫\n",
       "2         🤫\n",
       "         ..\n",
       "117371    💯\n",
       "117374    ✅\n",
       "117374    ✅\n",
       "117398    ✊\n",
       "117406    💪\n",
       "Name: emojis, Length: 45636, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_emojis[pd.notna(all_emojis)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emojis\n",
       "🙏    7286\n",
       "😂    3072\n",
       "🚜    2972\n",
       "✊    2411\n",
       "🌾    2363\n",
       "🏻    2080\n",
       "❤    1779\n",
       "🤣    1668\n",
       "🏽    1218\n",
       "👇    1108\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_emojis = all_emojis.value_counts().head(10)\n",
    "top_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Punto 3\n",
    "\n",
    "Para el punto 3 debemos obtener el top de usuarios mas influyendes, dependiendo del conteo de menciones que realiza el usuario en un tweet. Para ello se utiliza la columna **content** para obtener el contenido del tweet y contar el numero de menciones por tweets que realiza el usuario, adcional necesitaremos la columna **user** para obtener el nombre del usuario que realiza la mencion.\n",
    "\n",
    "Exploraremos la data y una posible solucion, en busca de anomalias en la data y conocimiento de la misma, este analisis continua siendo parte del analisis exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#ModiDontSellFarmers\\n#FarmersProtest https://t.co/uGQb1O5Jg9'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expresión regular para encontrar menciones\n",
    "mention_pattern = re.compile(r'@(\\w+)')\n",
    "df['mentions'] = df['content'].apply(lambda x: mention_pattern.findall(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Crear un contador para menciones hechas por cada usuario\n",
    "df['username'] = df['user'].apply(lambda x: x['username'])\n",
    "user_mention_counts = df.groupby('username')['mentions'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_count=df.explode('mentions').groupby('username')['mentions'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username\n",
       "loyal90901246      2479\n",
       "_gurviir           1826\n",
       "neetuanjle_nitu    1796\n",
       "JaiHind58236291    1696\n",
       "preetysaini321      930\n",
       "JoyJ69957841        915\n",
       "Gurpreetd86         844\n",
       "jot__b              840\n",
       "ScitaramSays        807\n",
       "mani262002          778\n",
       "Name: mentions, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_users = user_count.nlargest(10)\n",
    "list(top_users.items())\n",
    "top_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loyal90901246', 2479),\n",
       " ('_gurviir', 1826),\n",
       " ('neetuanjle_nitu', 1796),\n",
       " ('JaiHind58236291', 1696),\n",
       " ('preetysaini321', 930),\n",
       " ('JoyJ69957841', 915),\n",
       " ('Gurpreetd86', 844),\n",
       " ('jot__b', 840),\n",
       " ('ScitaramSays', 807),\n",
       " ('mani262002', 778)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(top_users.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUCION PROBLEMATICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from q1_time import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path= 'data/farmers-protest-tweets-2021-2-4.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 1 solucion:\n",
    "\n",
    "Para la solucion del punto 1 partimos del analisis generado en la seccion anterior, para ello lo primero que vamos a realizar es una funcion en pandas, teniendo en cuentas lo alineamientos anteriores, la cual me va obtener las 10 fechas con mas tweets, junto con el usuario que mas tweets realizo para esa fecha.\n",
    "\n",
    "Para la construccion de esta funcion se utilizo pandas debido a que es un libreria construida para trabajar con volumenes de datos, la idea de utilizar pandas es hacer uso de sus operaciones vectorizadas que se ejecutan rapidamente sobre el Dataframe, asi como la opcion de filtrado eficiente que ofrece esta libreria.\n",
    "\n",
    "La metodologia a seguir es la siguiente:\n",
    "\n",
    "1. Se construira una funcion generica para resolver la problematica en base al estudio realizado con anterioridad. Esta funcion a pesar de estar escrita con buenas practicas de uso de la libreria pandas, no esta optimizada y nos servira como base, para optimizar la funcion. \n",
    "\n",
    "2. Para la optimizacion de tiempo, se desglozara la funcion generica y se medira los tiempos de ejecucion en cada una de sus partes.\n",
    "\n",
    "3. Se haran mejoras de optimizacion de tiempo en la funcion, dando como resultado la funcion mejorada, comparada con la funcion original.\n",
    "\n",
    "4. Para la optimizacion de memoria al igual que para el anterior caso, se analizara y medira el uso de memoria en la funcion y se encontraran puntos a mejorar dentro de ella.\n",
    "\n",
    "5. Se aplicaran las mejoras de optimizacion de memoria en la funcion, dando como resultado la funcion optimizada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcion Base\n",
    "\n",
    "La funcion base se realizara y analizara dentro de este notebook, por facilidad de analisis, la funcion final, sera la que disponibilizaremos en el modulo **q1_time.py** y **q1_memory.py** respectivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q1_generic(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    df = pd.read_json(file_path, lines = True)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Tiempo de ejecución de lectura: {execution_time} segundos\")\n",
    "    \n",
    "    start_time2 = time.time()\n",
    "    df['newDate']= pd.to_datetime(df['date']).dt.date\n",
    "    \n",
    "    top_dates= df['newDate'].value_counts().head(10).index \n",
    "    result =[]\n",
    "    for date in top_dates:\n",
    "        #filtro el DataFrame con las fechas especificas\n",
    "        sub_df= df[df['newDate'] == date]\n",
    "\n",
    "        #encontrar el ususario con mas tweets para la fecha dada\n",
    "        top_user = sub_df['user'].apply(lambda x: x['username']).value_counts().idxmax()\n",
    "\n",
    "        result.append((date, top_user))\n",
    "    end_time2 = time.time()\n",
    "    execution_time2 = end_time2 - start_time2\n",
    "    print(f\"Tiempo de ejecución de procesamiento: {execution_time2} segundos\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizacion de tiempo de ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de lectura: 23.400452375411987 segundos\n",
      "Tiempo de ejecución de procesamiento: 0.3873133659362793 segundos\n"
     ]
    }
   ],
   "source": [
    "result= q1_generic(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de lectura: 10.13318157196045 segundos\n",
      "Tiempo de ejecución de procesamiento: 0.3672335147857666 segundos\n",
      "Tiempo de ejecución de lectura: 8.634846210479736 segundos\n",
      "Tiempo de ejecución de procesamiento: 0.3708813190460205 segundos\n",
      "Tiempo de ejecución de lectura: 7.701025724411011 segundos\n",
      "Tiempo de ejecución de procesamiento: 0.5652155876159668 segundos\n",
      "Tiempo de ejecución de lectura: 7.95357084274292 segundos\n",
      "Tiempo de ejecución de procesamiento: 0.3720278739929199 segundos\n",
      "Tiempo de ejecución de lectura: 7.6641247272491455 segundos\n",
      "Tiempo de ejecución de procesamiento: 0.37113070487976074 segundos\n",
      "Tiempo de ejecución de lectura: 7.648245573043823 segundos\n",
      "Tiempo de ejecución de procesamiento: 0.36950087547302246 segundos\n",
      "Tiempo de ejecución de lectura: 7.595869541168213 segundos\n",
      "Tiempo de ejecución de procesamiento: 0.37344861030578613 segundos\n",
      "Tiempo de ejecución de lectura: 7.617130517959595 segundos\n",
      "Tiempo de ejecución de procesamiento: 0.36878442764282227 segundos\n",
      "8.94 s ± 369 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit result2= q1_generic(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la prueba anterior tenemos una conclusion clara, y es que el mayor tiempo de procesamiento lo esta tomando la lectura del archivo. Esto hace sentido debido a que para el procesamiento estamos haciendo uso de operaciones vectorizadas de pandas, las cuales son muy optimas para trabajar con grandes volumenes de datos. Por otro lado para la lectura de archivo estamos usando la funcion **read_json** de pandas, la cual no es la manera mas optmiza de realizar la lectura.\n",
    "\n",
    "Para optimizar el tiempo de ejecucion realizaremos una optmizacion para la lectura del archivo, comparando varias tecnicas como el particionamiento y lectura por chunks, asi como tambien haremos uso de la libreria multiprocessing, para reaizar paralelismo en busca de utilizar al maximo los recursos de CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero es intentar leer el archivo con otras librerias como dask o vaex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import vaex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.91 s ± 303 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df = pd.read_json(file_path, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.3 s ± 647 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df2 = dd.read_json(file_path).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.97 s ± 610 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df3 = vaex.from_json(file_path, lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa el mejor performance para la lectura del json sigue siendo la libreria pandas, por consiguiente haremos uso de otras tecnicas.\n",
    "\n",
    "haremos la lectura en Chunks, para ello haremos uso de la libreria **orjson** que es significativamente mas rapida que la libreria **json** estandar, para cargar datos de tipo JSON esto reducidara el tiempo de lectura.\n",
    "\n",
    "Por otro lado hacer la lectura por chunks evita cargar el archivo en memoria de una vez, permitiendo el procesamiento mas rapido y eficiente. Procesar en chunks equilibria bien la carga de I/O y la memoria utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import orjson\n",
    "\n",
    "def load_json_in_chunks(file_path: str, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Carga un archivo JSON en chunks para optimizar el tiempo y uso de memoria.\n",
    "    \n",
    "    :param file_path: Ruta del archivo JSON.\n",
    "    :param chunk_size: Tamaño del chunk en número de líneas.\n",
    "    :return: Generador que produce chunks del archivo JSON.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            chunk.append(orjson.loads(line))\n",
    "            if len(chunk) == chunk_size:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de lectura: 6.024820327758789 segundos\n"
     ]
    }
   ],
   "source": [
    "import orjson\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "chunks = []\n",
    "for chunk in load_json_in_chunks(file_path):\n",
    "    df_chunk = pd.DataFrame(chunk)\n",
    "    chunks.append(df_chunk)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Tiempo de ejecución de lectura: {execution_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.77 s ± 318 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "chunks = []\n",
    "for chunk in load_json_in_chunks(file_path):\n",
    "    df_chunk = pd.DataFrame(chunk)\n",
    "    chunks.append(df_chunk)\n",
    "df2 = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, utilizando chunks y la libreria orjson, mejoramos significativamente el tiempo de lectura y ejecucion, en mas de un 50%.\n",
    "\n",
    "Ahora que pasa si hacemos uso del paralelismo, ejecutando en paralelo cada chunk??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Procesa un chunk de datos convirtiéndolo en un DataFrame y ajustando la columna de fecha.\n",
    "    \n",
    "    :param chunk: Lista de registros JSON.\n",
    "    :return: DataFrame procesado.\n",
    "    \"\"\"\n",
    "    df_chunk = pd.DataFrame(chunk)\n",
    "    df_chunk['date'] = pd.to_datetime(df_chunk['date']).dt.date\n",
    "    return df_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de lectura: 23.689802169799805 segundos\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "chunks = list(load_json_in_chunks(file_path))\n",
    "with Pool(cpu_count()) as pool:\n",
    "    df_chunks = pool.map(process_chunk, chunks)\n",
    "\n",
    "# Concatenar los DataFrames procesados\n",
    "df = pd.concat(df_chunks, ignore_index=True)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Tiempo de ejecución de lectura: {execution_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretacion**: a pesar de que el metodo de paralelismo por teoria, deberia darnos un mejor performance, la sobrecarga de la creacion de procesos y la comunicacion entre ellos, probablemente este causando un rendimiento peor que el esperado. Cabe recalcar que el uso de paralelismo es comendable cuando estamos trabajando con cantidades de datos muy grandes, ya hablando de big data, para este caso, a pesar de que el volumen de datos es alto, este no supera el orden de los MB.\n",
    "\n",
    "\n",
    "**Conclusion:** En conclusion el uso de multiprocessing en este caso no está dando los beneficios esperados y está introduciendo una sobrecarga significativa. La carga en chunks sin paralelismo es la más eficiente en términos de tiempo de ejecución total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from q1_time import q1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.17 s ± 314 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit result3= q1_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3 = q1_time(file_path)\n",
    "result3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizarion Memoria\n",
    "\n",
    "\n",
    "Para la optimizacion de memoria, realizaremos un analisis similar al anterior, partiendo de la funcion generic y encontrando puntos a mejorar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q1_generic(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    \n",
    "    df = pd.read_json(file_path, lines = True)\n",
    "    \n",
    "    df['newDate']= pd.to_datetime(df['date']).dt.date\n",
    "    \n",
    "    top_dates= df['newDate'].value_counts().head(10).index \n",
    "    result =[]\n",
    "    for date in top_dates:\n",
    "        #filtro el DataFrame con las fechas especificas\n",
    "        sub_df= df[df['newDate'] == date]\n",
    "\n",
    "        #encontrar el ususario con mas tweets para la fecha dada\n",
    "        top_user = sub_df['user'].apply(lambda x: x['username']).value_counts().idxmax()\n",
    "\n",
    "        result.append((date, top_user))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3188.89 MiB, increment: 2186.48 MiB\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "%memit q1_generic(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logica.\n",
    "\n",
    "Lo primero que realizaremos para la optimizacion de Memoria es en un inicio mantener la lectura del archivo por chunks, ya que este metodo optimiza en gran medida el uso de memoria al cargar el archivo por lotes, es una de las tecnicas mas conocidad para la optimizacion de Memoria.\n",
    "\n",
    "Segundo, Evaluaremos el tipo de datos del dataframe, del analisis exploratorio observamos que la mayoria de datos es object, un tipo de dato no muy eficiente si queremos optimizar memoria, para este caso podriamos utilizar un tipo de dato **category** por ejemplo.\n",
    "\n",
    "\n",
    "Tercero, evitaremos el uso de columnas que no se necesitan dentro de la solucion, asi como la creacion de columnas temporales, que tampoco son necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Procesa un chunk de datos convirtiéndolo en un DataFrame y ajustando la columna de fecha.\n",
    "    Convierte las columnas a tipos de datos más eficientes.\n",
    "    \n",
    "    :param chunk: Lista de registros JSON.\n",
    "    :return: DataFrame procesado.\n",
    "    \"\"\"\n",
    "    df_chunk = pd.DataFrame(chunk)\n",
    "    \n",
    "    # Convertir la columna 'date' a datetime y luego a solo fecha\n",
    "    df_chunk['date'] = pd.to_datetime(df_chunk['date']).dt.date\n",
    "    \n",
    "    # Convertir 'user' a un DataFrame separado\n",
    "    user_df = pd.json_normalize(df_chunk['user'])\n",
    "    \n",
    "    # Solo mantener las columnas necesarias\n",
    "    df_chunk = df_chunk[['date']]\n",
    "    # Optimizar los tipos de datos\n",
    "    df_chunk['user'] = user_df['username'].astype('category')\n",
    "    \n",
    "    \n",
    "    return df_chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for chunk in load_json_in_chunks(file_path):\n",
    "    df_chunk = process_chunk(chunk)\n",
    "    chunks.append(df_chunk)\n",
    "df = pd.concat(chunks, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-24</td>\n",
       "      <td>ArjunSinghPanam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-24</td>\n",
       "      <td>PrdeepNain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-24</td>\n",
       "      <td>parmarmaninder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-24</td>\n",
       "      <td>anmoldhaliwal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-24</td>\n",
       "      <td>KotiaPreet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date             user\n",
       "0  2021-02-24  ArjunSinghPanam\n",
       "1  2021-02-24       PrdeepNain\n",
       "2  2021-02-24   parmarmaninder\n",
       "3  2021-02-24    anmoldhaliwal\n",
       "4  2021-02-24       KotiaPreet"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta optimizacion ya podemos crear una funcion temporal para analizar su optimizacion de memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q1_memory_temporal(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    \n",
    "    chunks = []\n",
    "    for chunk in load_json_in_chunks(file_path):\n",
    "        df_chunk = process_chunk(chunk)\n",
    "        chunks.append(df_chunk)\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    df['user'] = df['user'].astype('category')\n",
    "    \n",
    "    # Obtener las 10 fechas con más tweets\n",
    "    top_dates = df['date'].value_counts().head(10).index\n",
    "    \n",
    "    result = []\n",
    "    for date in top_dates:\n",
    "        sub_df = df[df['date'] == date]\n",
    "        \n",
    "        top_user = sub_df['user'].value_counts().idxmax()\n",
    "        \n",
    "        result.append((date, top_user))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1232.96 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_memory_temporal(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Conclusion:\n",
    "\n",
    "Podemos observar una mejora significativa del uso de memoria, de aproximadamente el 30%  pasando de 3188 MB a 1200 MB, Mostrando la viabilidad de este desarrollo. La solucion final la podremos encontrar en el modulo **q1_memory.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from q1_memory import q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1232.99 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 2 Solucion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la solucion del punto 2 utilizaremos una logica similar a la que usamos en el punto 1, vamos a crear una funcion generica a partir del analisis realizado en la exploracion de la data, esta funcion generica tendra la logica que resolvera el desafio propuesto, pero esta lejos de ser optimizada tanto en tiempos como en memoria.\n",
    "\n",
    "Para ello mediremos el tiempo de ejecucio y el uso de memoria en la funcion generacia, y buscaremos optimizar dicha funcion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "# Función para extraer emojis\n",
    "def extract_emojis(text):\n",
    "    return [char for char in text if emoji.is_emoji(char)]\n",
    "\n",
    "\n",
    "def q2_generic(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    df = pd.read_json(file_path, lines = True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Tiempo de ejecución de lectura: {execution_time} segundos\")\n",
    "    \n",
    "    start_time2 = time.time()\n",
    "    # Extraer y contar emojis\n",
    "    df['emojis'] = df['content'].apply(extract_emojis)\n",
    "    all_emojis = df['emojis'].explode()\n",
    "    top_emojis = all_emojis.value_counts().head(10)\n",
    "    \n",
    "    end_time2 = time.time()\n",
    "    execution_time2 = end_time2 - start_time2\n",
    "    print(f\"Tiempo de ejecución de proceso: {execution_time2} segundos\")\n",
    "    return list(top_emojis.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de lectura: 11.100608825683594 segundos\n",
      "Tiempo de ejecución de proceso: 4.589721918106079 segundos\n",
      "CPU times: user 14.9 s, sys: 1.47 s, total: 16.4 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "emojis = q2_generic(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 7286),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('✊', 2411),\n",
       " ('🌾', 2363),\n",
       " ('🏻', 2080),\n",
       " ('❤', 1779),\n",
       " ('🤣', 1668),\n",
       " ('🏽', 1218),\n",
       " ('👇', 1108)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizacion de tiempo de ejecucion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la ejecucion de la funcion generica, podemos deducir los siguiente puntos:\n",
    "\n",
    "\n",
    "1. Al igual como sucedio con el punto 1, el mayor tiempo de ejecucion lo esta tomando la lectura del archivo, el cual se esta leyendo haciendo uso de la biblioteca python. Para la optimiazacion de la lectura, haremos uso del analisis previamente realizado, en el cual se concluyo que la manera mas optima de leer el archivo es usando chunks.\n",
    "\n",
    "2. Podemos observar un tiempo de ejecucion considerable en el proceso, esto es debido a que a pesar de que estamos haciendo buen uso de funciones vectorizadas de pandas, tambien estamos haciendo uso de la libreria emojis, la cual por cada tweet me recorre los caracteres en busca de un emoji, haciendo de este un proceso lento y tedioso si tenemos en cuenta la cantidad de data que tenemos.\n",
    "\n",
    "**posible solucion apartado 2:** Sabemos que la extraccion de emojis con **emoji.is_emoji** en cada carecter es inificiente, esta problematica se podria solucionar utilizando expresiones regualares que detecten emojis en el textos, haciendolo mas eficiente debido a que no recorre caracter por caracter. Pero si somos fieles al analisis realizado con anterioridad, nos damos cuenta que el uso de expresiones regulares no es una solucion viable en cuanto a calidad del dato y resultado, debido a que tiene problemas para identificar algunos emojis especificos, emojis consecutivos y demas.\n",
    "\n",
    "En base al analisis realizado en la etapa de exploracion, nos vemos en la necesidad de optimizar el codigo con la biblioteca emoji, debido a que tiene un mejor performance o accuracy. Para ello La función **extract_emojis** se aplica a cada chunk de datos de manera vectorizada utilizando pandas. Esta metodología es eficiente porque el uso de orjson para leer el archivo JSON en chunks reduce significativamente el tiempo de lectura y manejo de memoria, mientras que pandas permite aplicar la función de extracción de emojis de manera vectorizada, mejorando la velocidad de procesamiento sin comprometer la precisión. Esta combinación asegura un rendimiento óptimo al manejar grandes volúmenes de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Procesa un chunk de datos convirtiéndolo en un DataFrame y extrayendo emojis.\n",
    "    \n",
    "    :param chunk: Lista de registros JSON.\n",
    "    :return: DataFrame procesado con una columna de emojis.\n",
    "    \"\"\"\n",
    "    df_chunk = pd.DataFrame(chunk)\n",
    "    \n",
    "    # Extraer emojis del contenido\n",
    "    df_chunk['emojis'] = df_chunk['content'].apply(extract_emojis)\n",
    "    \n",
    "    return df_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q2_time_temporal(file_path):\n",
    "  \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Procesar los chunks y concatenar\n",
    "    chunks = []\n",
    "    for chunk in load_json_in_chunks(file_path):\n",
    "        df_chunk = process_chunk(chunk)\n",
    "        chunks.append(df_chunk)\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Tiempo de ejecución de lectura y procesamiento inicial: {execution_time} segundos\")\n",
    "    \n",
    "    start_time2 = time.time()\n",
    "    # Explode y contar emojis\n",
    "    all_emojis = df['emojis'].explode()\n",
    "    top_emojis = all_emojis.value_counts().head(10)\n",
    "    \n",
    "    end_time2 = time.time()\n",
    "    execution_time2 = end_time2 - start_time2\n",
    "    print(f\"Tiempo de ejecución de conteo: {execution_time2} segundos\")\n",
    "    \n",
    "    return list(top_emojis.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de lectura y procesamiento inicial: 9.614159345626831 segundos\n",
      "Tiempo de ejecución de conteo: 0.03318309783935547 segundos\n",
      "CPU times: user 9.92 s, sys: 237 ms, total: 10.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "emojis2= q2_time_temporal(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conlclusion:\n",
    "\n",
    "La optimización implementada redujo significativamente el tiempo de lectura del archivo JSON y el tiempo total de ejecución. La lectura en chunks con orjson y el procesamiento vectorizado con pandas mejoraron el rendimiento, reduciendo el tiempo total de ejecución de 16.4 segundos a 10.2 segundos. La combinación de estas técnicas no solo hizo el proceso más rápido sino también más eficiente en términos de manejo de memoria, manteniendo la precisión en la detección de emojis.\n",
    "\n",
    "Dado lo anterior podemos implementar una solucion general en el modulo **q2_time.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from q2_time import q2_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.71 s, sys: 448 ms, total: 10.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_emoji = q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 s ± 375 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q2_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizacion Memoria q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar al anterior caso, partiremos de la optimizacion de la funcion generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "# Función para extraer emojis\n",
    "def extract_emojis(text):\n",
    "    return [char for char in text if emoji.is_emoji(char)]\n",
    "\n",
    "\n",
    "def q2_generic(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \n",
    "    df = pd.read_json(file_path, lines = True)\n",
    "   \n",
    "    # Extraer y contar emojis\n",
    "    df['emojis'] = df['content'].apply(extract_emojis)\n",
    "    all_emojis = df['emojis'].explode()\n",
    "    top_emojis = all_emojis.value_counts().head(10)\n",
    "\n",
    "    return list(top_emojis.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3199.75 MiB, increment: 2397.07 MiB\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "%memit q2_generic(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la optimizacion de memoria, utilizaremos el siguiente proceso:\n",
    "    \n",
    "1. Lectura selectiva de columnas por chunk: Vamos a leer solo la columna content utilizando orjson y procesaremos solo los datos necesarios.\n",
    "\n",
    "2. Procesamiento en Chunks: Mantener la lectura en chunks para no cargar todo el archivo en memoria de una vez.\n",
    "\n",
    "3. Descarte de Datos Innecesarios: Descartar columnas innecesarias inmediatamente después de su uso reduciendo el uso de memoria. En este caso eliminaremos la columna 'content'\n",
    "\n",
    "4. Uso de pandas y Explosión de Emojis: Utilizamos las capacidades de pandas para operaciones vectorizadas es eficiente y manejable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Procesa un chunk de datos extrayendo solo la columna 'content' y los emojis.\n",
    "    \n",
    "    :param chunk: Lista de registros JSON.\n",
    "    :return: DataFrame procesado con una columna de emojis.\n",
    "    \"\"\"\n",
    "    contents = [record['content'] for record in chunk]\n",
    "    df_chunk = pd.DataFrame(contents, columns=['content'])\n",
    "    \n",
    "    # Extraer emojis del contenido\n",
    "    df_chunk['emojis'] = df_chunk['content'].apply(extract_emojis)\n",
    "    \n",
    "    # Descartar la columna 'content' para reducir el uso de memoria\n",
    "    df_chunk.drop(columns=['content'], inplace=True)\n",
    "    \n",
    "    return df_chunk\n",
    "\n",
    "def q2_memory_temporal(file_path: str) -> List[Tuple[str, int]]:\n",
    "  \n",
    "    # Procesar los chunks y concatenar solo la columna de emojis\n",
    "    chunks = []\n",
    "    for chunk in load_json_in_chunks(file_path):\n",
    "        df_chunk = process_chunk(chunk)\n",
    "        chunks.append(df_chunk)\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    # Explode y contar emojis\n",
    "    all_emojis = df['emojis'].explode()\n",
    "    top_emojis = all_emojis.value_counts().head(10)\n",
    "    \n",
    "    return list(top_emojis.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1066.84 MiB, increment: 1.14 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q2_memory_temporal(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "Podemos observar una mejora significativa del uso de memoria, de aproximadamente el 33%  pasando de 3200 MB a 1066 MB, Mostrando la viabilidad de este desarrollo. La solucion final la podremos encontrar en el modulo **q2_memory.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from q2_memory import q2_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1065.98 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 3 Solucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def q3_generic(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    df = pd.read_json(file_path, lines = True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Tiempo de ejecución de lectura: {execution_time} segundos\")\n",
    "    \n",
    "    start_time2 = time.time()\n",
    "    # Expresión regular para encontrar menciones\n",
    "    mention_pattern = re.compile(r'@(\\w+)')\n",
    "    df['mentions'] = df['content'].apply(lambda x: mention_pattern.findall(x))\n",
    "    \n",
    "    # Crear un contador para menciones hechas por cada usuario\n",
    "    df['username'] = df['user'].apply(lambda x: x['username'])\n",
    "    user_count=df.explode('mentions').groupby('username')['mentions'].count()\n",
    "    top_users = user_count.nlargest(10)\n",
    "\n",
    "    end_time2 = time.time()\n",
    "    execution_time2 = end_time2 - start_time2\n",
    "    print(f\"Tiempo de ejecución de proceso: {execution_time2} segundos\")\n",
    "    \n",
    "    return list(top_users.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de lectura: 13.685860872268677 segundos\n",
      "Tiempo de ejecución de proceso: 1.5820708274841309 segundos\n",
      "CPU times: user 14.5 s, sys: 1.62 s, total: 16.1 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mentions = q3_generic(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de lectura: 19.05904531478882 segundos\n",
      "Tiempo de ejecución de proceso: 1.495091438293457 segundos\n",
      "peak memory: 4480.96 MiB, increment: 1166.48 MiB\n"
     ]
    }
   ],
   "source": [
    "# Medicion uso de memoria\n",
    "%memit q3_generic(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis:\n",
    "\n",
    "Del punto 1 y 2 hemos aprendido varias cosas a cerca de la data y como optimizarla, asi como tambien diferentes tecnicas de optimizacion tanto de memoria como de tiempo. Para este punto vamos a utilizar todas estas tecnicas que me resultaron exitosas en los puntos anteriores, para abordar la solucion de manera mas eficiente.\n",
    "\n",
    "Primero que todo el tema de la lectura del archivo: En todos los 3 casos hemos visto que el mayor tiempo de ejecucion lo toma la lectura del archivo, y desde el analisis exhaustivo realizado en el punto 1, nos dimos cuenta que la mejor solucion para optimizar este tiempo, es el uso de lectura por chunks. Es decir, para este punto continuaremos con la misma solucion.\n",
    "\n",
    "Segundo, nos hemos dado cuenta que no toda la data es necesaria para resolver las problematicas, y que podemos leer el json por chunks, seleccionando las columnas especificas que se necesitan para cada solucion. En este caso, para el punto 3, las columnas *content* & *user*.\n",
    "\n",
    "Tercero, hemos notado que el uso de funciones vectorizadas es la mejor opcion de optimizacion de tiempo y memoria que podemos tener cuando trabajamos con datos en pandas. En este caso tenemos multiples usos de la funcion **apply** lo cual no es muy optimo. Para este caso, buscare optimizar el uso de este apply en la lectura del json por chunks, al momento de procesarlos.\n",
    "\n",
    "\n",
    "La solucion para la optimizacion de tiempo es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Expresión regular para encontrar menciones\n",
    "mention_pattern = re.compile(r'@(\\w+)')\n",
    "\n",
    "def extract_mentions(text):\n",
    "    return mention_pattern.findall(text)\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Procesa un chunk de datos extrayendo solo las columnas 'content' y 'user' y las menciones.\n",
    "    \n",
    "    :param chunk: Lista de registros JSON.\n",
    "    :return: DataFrame procesado con una columna de menciones.\n",
    "    \"\"\"\n",
    "    contents = [(record['content'], record['user']['username']) for record in chunk]\n",
    "    df_chunk = pd.DataFrame(contents, columns=['content', 'username'])\n",
    "    \n",
    "    # Extraer menciones del contenido\n",
    "    df_chunk['mentions'] = df_chunk['content'].apply(extract_mentions)\n",
    "    \n",
    "    return df_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la funcion process_chunk, con la cual aprovechamos el uso de lectura por chunk para extraer solo la columna 'content' & 'user' y adicional, extraemos y creamos la nueva columna mentions, la cual contiene una lista de las menciones realizadas por un usuario para ese tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q3_time_temporal(file_path: str) -> List[Tuple[str, int]]:\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Procesar los chunks y concatenar\n",
    "    chunks = []\n",
    "    for chunk in load_json_in_chunks(file_path):\n",
    "        df_chunk = process_chunk(chunk)\n",
    "        chunks.append(df_chunk)\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Tiempo de ejecución de lectura y procesamiento inicial: {execution_time} segundos\")\n",
    "    \n",
    "    start_time2 = time.time()\n",
    "    # Crear un contador para menciones hechas por cada usuario\n",
    "    user_count = df.explode('mentions').groupby('username')['mentions'].count()\n",
    "    top_users = user_count.nlargest(10)\n",
    "    \n",
    "    end_time2 = time.time()\n",
    "    execution_time2 = end_time2 - start_time2\n",
    "    print(f\"Tiempo de ejecución de conteo: {execution_time2} segundos\")\n",
    "    \n",
    "    return list(top_users.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución de lectura y procesamiento inicial: 6.7898993492126465 segundos\n",
      "Tiempo de ejecución de conteo: 0.2587771415710449 segundos\n",
      "CPU times: user 6.98 s, sys: 155 ms, total: 7.13 s\n",
      "Wall time: 7.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mentions2 = q3_time_temporal(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Podemos ver una gran mejora en el tiempo de ejecucion utilizando la optimizacion propuesta por chunk, y el uso adecuado de las funciones vectorizadas de pandas. La reduccion de tiempo fue de mas del 50%, pasando de un wall time de 17 s a un wall time de 7,13 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from q3_time import q3_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.91 s, sys: 475 ms, total: 7.39 s\n",
      "Wall time: 7.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mentions3 = q3_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizacion de Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la optimizacion de memoria al igual que en los casos anteriores, haremos uso de la lectura por chunks como lo hemos trabajado, Adicional usaremos solo las columnas y datos necesrarios para resolver la problematica, reduciendo significativamente el uso de memoria al no usar datos innecesarios y finalmente convertiremos los datos de tipo **object** a **category** el cual es una buena practica para disminuir significativamente el uso de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Expresión regular para encontrar menciones\n",
    "mention_pattern = re.compile(r'@(\\w+)')\n",
    "\n",
    "def extract_mentions(text):\n",
    "    return mention_pattern.findall(text)\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Procesa un chunk de datos extrayendo solo las columnas 'content' y 'user' y las menciones.\n",
    "    \n",
    "    :param chunk: Lista de registros JSON.\n",
    "    :return: DataFrame procesado con una columna de menciones.\n",
    "    \"\"\"\n",
    "    contents = [(record['content'], record['user']['username']) for record in chunk]\n",
    "    df_chunk = pd.DataFrame(contents, columns=['content', 'username'])\n",
    "    \n",
    "    # Extraer menciones del contenido\n",
    "    df_chunk['mentions'] = df_chunk['content'].apply(extract_mentions)\n",
    "    \n",
    "    # Convertir la columna 'username' a tipo 'category' para ahorrar memoria\n",
    "    df_chunk['username'] = df_chunk['username'].astype('category')\n",
    "    \n",
    "    # Descartar la columna 'content' para reducir el uso de memoria\n",
    "    df_chunk.drop(columns=['content'], inplace=True)\n",
    "    \n",
    "    return df_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el procesamiento de chunks estamos creando 2 nuevas columnas, la primera es la columna **mentions** la cual contiene las menciones realizadas en un tweet, y segundo la columna **username** la cual es creada a partir de la columna **user** para obtener el usuario que realizo las menciones.\n",
    "\n",
    "Finalmente eliminamos la columna **content** debido a que no va ser usada en los procesos posteriores, optimizando asi el uso de memoria, mantiendo solo la data 100% necesaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q3_memory_temporal(file_path: str) -> List[Tuple[str, int]]:\n",
    "\n",
    "    # Procesar los chunks y concatenar solo las columnas necesarias\n",
    "    chunks = []\n",
    "    for chunk in load_json_in_chunks(file_path):\n",
    "        df_chunk = process_chunk(chunk)\n",
    "        chunks.append(df_chunk)\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    # Explode y contar menciones\n",
    "    all_mentions = df['mentions'].explode()\n",
    "    user_count = all_mentions.groupby(df['username']).count()\n",
    "    top_users = user_count.nlargest(10)\n",
    "    \n",
    "    return list(top_users.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "peak memory: 531.88 MiB, increment: 145.05 MiB\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "# Medicion uso de memoria\n",
    "%memit q3_memory_temporal(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "Podemos observar una optimizacion significativa del uso de memoria, pasando de 4480 MB a tan solo 531 MB, concluyendo que las tecnicas utilizadas dentro de todo el proceso son optimas y aplican muy bien al caso de estudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from q3_memory import q3_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 531.12 MiB, increment: 73.78 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu113.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu113:m123"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
